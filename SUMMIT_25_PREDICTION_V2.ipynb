{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "7nmfzocyuyhmzodta52q",
   "authorId": "8329542926316",
   "authorName": "JASON_MFG",
   "authorEmail": "jason.drew@snowflake.com",
   "sessionId": "2b004e99-e427-4950-90c9-feb23d09b979",
   "lastEditTime": 1747349628265
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d46cfa4-eb80-4fa1-8d32-33bce987790e",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Truck Failure Prediction ML Pipeline\n# =================================\n# This notebook demonstrates an end-to-end ML pipeline for predicting truck failures using Snowflake's ML capabilities. We'll predict which trucks are likely to fail in the next 12 hours based on sensor data."
  },
  {
   "cell_type": "markdown",
   "id": "757b5be6-d13e-4b27-b74b-91b8010729ff",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "# --------------------------------\n# Config - Set all parameters here\n# --------------------------------"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "DATABASE_NAME = \"SUMMIT_25\"\nSCHEMA_NAME = \"ASSET_HEALTH\"\nWAREHOUSE_NAME = \"MEDIUM\"\nMODEL_NAME = \"ASSET_HEALTH_12HOUR_FAILURE_PREDICTION\"\nMODEL_VERSION = \"XGB_V1\"\nRAW_DATA_TABLE = \"TURBO_HISTORY_DATA\"\nPRODUCTION_DATA_TABLE = \"TURBO_DATA_PRODUCTION\"\nPREDICTION_OUTPUT_TABLE = \"TURBO_DATA_PREDICTIONS_NEW\"\nTRAIN_TEST_SPLIT_DATE = \"2025-03-20 00:00:00\"\n\n# Import required packages\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport shap\nfrom snowflake.snowpark.functions import col, lag, avg, stddev, min as sf_min, max as sf_max, hour\nfrom snowflake.snowpark.window import Window\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\nfrom snowflake.ml.registry import Registry\nfrom snowflake.snowpark import Session, DataFrame, Window, WindowSpec\nimport snowflake.snowpark.functions as F\n\n# Get active session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8f72482b-0cc1-4b5d-afe2-2bac0790654c",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "# --------------------\n# 1. Data Exploration\n# --------------------"
  },
  {
   "cell_type": "code",
   "id": "c98a159f-0ef7-4bd2-be10-5e03383a45fb",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "# First, let's examine our raw sensor data to understand what we're working with\nprint(\"Exploring raw sensor data...\")\n\n# Load and display raw data\ndf = session.table(RAW_DATA_TABLE)\ndf = df.with_column(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\ndf = df.with_column(\"truck_id\", col(\"truck_id\").cast(\"integer\"))\nprint(\"Raw data sample:\")\ndf.show(5)\n\nprint(\"Data columns:\")\nfor column in df.columns:\n    print(f\"- {column}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58c745d6-1a60-4031-99ac-c9a78991bfe6",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "# ---------------------------\n# 2. Feature Engineering\n# ---------------------------"
  },
  {
   "cell_type": "code",
   "id": "0b7895a2-42eb-41f3-9b73-515c74ed9808",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# We'll create features from sensor data to help predict failures\nprint(\"\\nEngineering features from sensor data...\")\n\n# Helper function for feature engineering\ndef create_lag_features(df, columns, window):\n    \"\"\"Create lag features for the specified columns\"\"\"\n    result_df = df\n    for column in columns:\n        result_df = result_df.with_column(f\"{column}_lag1\", lag(col(column)).over(window))\n    return result_df\n\ndef create_rolling_stats(df, columns, window, stat_type=\"avg\"):\n    \"\"\"Create rolling statistics for the specified columns\"\"\"\n    result_df = df\n    for column in columns:\n        if stat_type == \"avg\":\n            result_df = result_df.with_column(f\"{column}_avg_1hr\", avg(col(column)).over(window))\n        elif stat_type == \"min\":\n            result_df = result_df.with_column(f\"{column}_min_1hr\", sf_min(col(column)).over(window))\n        elif stat_type == \"max\":\n            result_df = result_df.with_column(f\"{column}_max_1hr\", sf_max(col(column)).over(window))\n    return result_df\n\ndef create_delta_features(df, columns):\n    \"\"\"Create delta features (current - previous) for the specified columns\"\"\"\n    result_df = df\n    for column in columns:\n        result_df = result_df.with_column(f\"delta_{column}\", col(column) - col(f\"{column}_lag1\"))\n    return result_df\n\n# Define window for time-based operations\nw = Window.partition_by(\"truck_id\").order_by(\"timestamp\")\nrolling_window = w.rows_between(-12, 0)  # 12 hour rolling window\n\n# Sensor columns to use for feature engineering\nsensor_columns = [\"exhaust_gas_temp\", \"oil_pressure\", \"boost_pressure\", \n                 \"oil_contamination\", \"engine_boost_ratio\"]\n\n# 1. Create lag features (previous values)\ndf = create_lag_features(df, sensor_columns, w)\n\n# 2. Create rolling average features\ndf = create_rolling_stats(df, sensor_columns, rolling_window, \"avg\")\n\n# 3. Create min/max features for specific sensors\ntemp_pressure_columns = [\"exhaust_gas_temp\", \"oil_pressure\"]\ndf = create_rolling_stats(df, temp_pressure_columns, rolling_window, \"min\")\ndf = create_rolling_stats(df, temp_pressure_columns, rolling_window, \"max\")\n\n# 4. Create rate of change features\ndf = create_delta_features(df, sensor_columns)\n\n# 5. Add time-based features\ndf = df.with_column(\"hour_of_day\", hour(col(\"timestamp\")))\n\n# 6. Remove label columns from feature set\ndf = df.drop([\"PART_FAILURE_NEXT_12HR\", \"ACTUAL_FAILURE_EVENT\"])\n\nprint(\"Engineered features sample:\")\ndf.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d4a8a75-9862-4857-9784-26c2f0362c43",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "# -----------------------------------\n# 3. Feature Store Creation & Registration\n# -----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "f4065f79-cb53-42bf-9873-f5ce5811572d",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# We'll register our features in Snowflake's Feature Store for reuse\nprint(\"\\nRegistering features in Feature Store...\")\n\n# Create feature store\nfs = FeatureStore(\n    session=session,\n    database=DATABASE_NAME,\n    name=SCHEMA_NAME,\n    default_warehouse=WAREHOUSE_NAME,\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\n# Create or retrieve entity definition\ntry:\n    # Try to retrieve existing entity\n    truck_entity = fs.get_entity('Sensor_Data')\n    print('Retrieved existing entity')\nexcept:\n    # Define new entity if it doesn't exist\n    truck_entity = Entity(\n        name=\"Sensor_Data\",\n        join_keys=[\"truck_id\"]\n    )\n    \n    fs.register_entity(truck_entity)\n    print(\"Registered new entity\")\n\n# Create feature view\ntruck_feature_view = FeatureView(\n    name=\"truck_sensors_feature_view_train\",\n    entities=[truck_entity],\n    feature_df=df,\n    timestamp_col=\"timestamp\",\n    refresh_freq=\"5 minutes\",\n    desc=\"Lagged sensor features for truck failure predictions\"\n)\n\n# Register feature view\nregistered_fv = fs.register_feature_view(\n    feature_view=truck_feature_view,\n    version=\"v1\",\n    block=True,\n    overwrite=True\n)\n\nprint(f\"Registered feature view: {registered_fv.name} (v{registered_fv.version})\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d92761a-2cab-4c08-8f66-aeb4415c3a13",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "# -----------------------------------\n# 4. Generate Training Dataset\n# -----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "564c069d-c1ba-416c-941d-74830b74ae02",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# Create dataset for training by joining features with labels\nprint(\"\\nGenerating training dataset...\")\n\n# Get entity data with labels\nentity_df = session.table(RAW_DATA_TABLE).select(\"truck_id\", \"timestamp\", \"PART_FAILURE_NEXT_12HR\")\nentity_df = entity_df.with_column(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n\n# Generate training set from feature store\ntraining_df = fs.generate_training_set(\n    features=[registered_fv],\n    spine_df=entity_df,\n    spine_timestamp_col=\"timestamp\",\n    spine_label_cols=[\"PART_FAILURE_NEXT_12HR\"]\n)\n\nprint(\"Training dataset sample:\")\ntraining_df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae0764f2-99db-4653-be84-53514dfb348c",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 5. Create Test/Train Split\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "2e42fc85-3481-480e-be8d-54d4c02d05e1",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "print(f\"\\nSplitting data into train/test sets at {TRAIN_TEST_SPLIT_DATE}...\")\n\n# Split based on timestamp\ntrain_df = training_df.filter(col(\"timestamp\") < TRAIN_TEST_SPLIT_DATE) \\\n           .with_column(\"PART_FAILURE_NEXT_12HR\", col(\"PART_FAILURE_NEXT_12HR\").cast(\"integer\")) \\\n           .dropna()\ntest_df = training_df.filter(col(\"timestamp\") >= TRAIN_TEST_SPLIT_DATE).dropna()\n\nprint(f\"Train set size: {train_df.count()} rows\")\nprint(f\"Test set size: {test_df.count()} rows\")\n\n# Define feature and label columns\nfeature_cols = [\n    \"EXHAUST_GAS_TEMP\", \"OIL_PRESSURE\", \"BOOST_PRESSURE\", \"OIL_CONTAMINATION\", \n    \"ENGINE_BOOST_RATIO\", \"AMBIENT_TEMP\", \"RPM\", \"SPEED\", \"GPS_LAT\", \"GPS_LON\",\n    \"EXHAUST_GAS_TEMP_LAG1\", \"OIL_PRESSURE_LAG1\", \"BOOST_PRESSURE_LAG1\", \n    \"OIL_CONTAMINATION_LAG1\", \"ENGINE_BOOST_RATIO_LAG1\",\n    \"EXHAUST_GAS_TEMP_AVG_1HR\", \"OIL_PRESSURE_AVG_1HR\", \"BOOST_PRESSURE_AVG_1HR\", \n    \"OIL_CONTAMINATION_AVG_1HR\", \"ENGINE_BOOST_RATIO_AVG_1HR\",\n    \"EXHAUST_GAS_TEMP_MIN_1HR\", \"EXHAUST_GAS_TEMP_MAX_1HR\", \"OIL_PRESSURE_MIN_1HR\", \n    \"OIL_PRESSURE_MAX_1HR\",\n    \"DELTA_EXHAUST_GAS_TEMP\", \"DELTA_OIL_PRESSURE\", \"DELTA_BOOST_PRESSURE\", \n    \"DELTA_OIL_CONTAMINATION\", \"DELTA_ENGINE_BOOST_RATIO\",\n    \"HOUR_OF_DAY\"\n]\n\nlabel_col = \"PART_FAILURE_NEXT_12HR\"\n\n\n# Fix for Decimal Type Conversion Warnings\ndef explicitly_cast_decimal_columns(df):\n    \"\"\"\n    Explicitly cast Decimal columns to Double to avoid automatic conversion warnings\n    \"\"\"\n    from snowflake.snowpark.types import DoubleType\n    \n    # Get the schema to identify Decimal columns\n    schema = df.schema\n    \n    # Loop through all columns and explicitly cast Decimal types to Double\n    for field in schema.fields:\n        if \"DECIMAL\" in str(field.datatype).upper():\n            df = df.withColumn(field.name, df[field.name].cast(DoubleType()))\n    \n    return df\n\n# Apply explicit casting to avoid conversion warnings\ntrain_df = explicitly_cast_decimal_columns(train_df)\ntest_df = explicitly_cast_decimal_columns(test_df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cadd0b27-df92-4f22-962a-9f58caba460b",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 6. Train Model\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "a6697afe-aa55-4d4f-aa6a-3c147837f0fe",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "print(\"\\nTraining XGBoost classifier model...\")\n\n# Import warnings to suppress specific warnings\nimport warnings\n\n# Create and train XGBoost model\nmodel = XGBClassifier(\n    input_cols=feature_cols,\n    label_cols=[label_col],\n    output_cols=[\"PREDICTION\"]\n)\n\n# Suppress only the specific truncation warning during fitting\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", message=\".*truncation happened before inferring signature.*\")\n    model.fit(train_df)\n    print(\"Model training complete on full dataset!\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f7f0fa6-4593-4bdd-b491-39ccb8138425",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 7. Evaluate Model\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "b8ff4431-9e9b-44c0-8495-84567c0596fd",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "print(\"\\nEvaluating model on test data...\")\n\n# Make predictions on test data\npred_df = model.predict(test_df)\npred_df.show(5)\n\n# Calculate metrics\naccuracy_score_test = accuracy_score(\n    df=pred_df, \n    y_true_col_names=[label_col], \n    y_pred_col_names=[\"PREDICTION\"]\n)\nf1_score_test = f1_score(\n    df=pred_df, \n    y_true_col_names=[label_col], \n    y_pred_col_names=[\"PREDICTION\"]\n)\nrecall_score_test = recall_score(\n    df=pred_df, \n    y_true_col_names=[label_col], \n    y_pred_col_names=[\"PREDICTION\"]\n)\nprecision_score_test = precision_score(\n    df=pred_df, \n    y_true_col_names=[label_col], \n    y_pred_col_names=[\"PREDICTION\"]\n)\n\nprint(f\"Test Metrics:\")\nprint(f\"- Accuracy: {accuracy_score_test:.4f}\")\nprint(f\"- F1 Score: {f1_score_test:.4f}\")\nprint(f\"- Recall: {recall_score_test:.4f}\")\nprint(f\"- Precision: {precision_score_test:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a5bf1ff-5f0b-4705-841a-94842b2d5783",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 8. Register Model in Model Registry\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "5af4632f-64d6-4d92-8aff-e08231e4180e",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "print(\"\\nRegistering model in Snowflake Model Registry...\")\n\n# Import the Registry class\nfrom snowflake.ml.registry import Registry\n\n# Create the registry object\nregistry = Registry(session=session)\n\n# Create a small sample for registration\nregistration_sample = train_df.drop([\"TRUCK_ID\", \"TIMESTAMP\", label_col]).limit(100)\n\n# Import warnings to suppress specific warnings\nimport warnings\n\n# Generate a timestamp-based version to ensure uniqueness\nimport datetime\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nnew_version = f\"XGB_V1_{timestamp}\"\n\n# Log model to registry with metrics and new version name\nprint(f\"Logging model: {MODEL_NAME} version: {new_version}\")\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", message=\".*truncation happened before inferring signature.*\")\n    warnings.filterwarnings(\"ignore\", message=\".*Providing model signature for Snowpark ML Modeling model is not required.*\")\n    warnings.filterwarnings(\"ignore\", message=\".*`relax_version` is not set and therefore defaulted to True.*\")    \n    mv_base = registry.log_model(\n        model_name=MODEL_NAME,\n        model=model,\n        version_name=new_version,  # Use new unique version name\n        sample_input_data=registration_sample,\n        comment=\"XGBoost Forecasting Model\"\n    )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c853ba07-9010-4e8d-b92c-e5377dcb1ef3",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 9. Analyze Feature Importance\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "b692692c-cda4-41b3-a286-fe772a7dfc66",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "print(\"\\nAnalyzing feature importance with SHAP values...\")\n\n# Create sample for SHAP analysis \ntest_pd = test_df.to_pandas()\ntest_pd_sample = test_pd.sample(n=min(2500, len(test_pd)), random_state=100).reset_index(drop=True)\n\n# Compute Shapley values\nbase_shap_pd = mv_base.run(test_pd_sample, function_name=\"explain\")\n\n# Convert to proper format for visualization\nshap_values_np = np.array(base_shap_pd.astype(float))\nfeature_names = test_pd_sample.drop(columns=[label_col, \"TIMESTAMP\", \"TRUCK_ID\"]).columns\n\n# Create SHAP values object\nshap_values_obj = shap._explanation.Explanation(\n    values=shap_values_np,\n    feature_names=feature_names,\n    data=test_pd_sample[feature_names].values\n)\n\nprint(\"Top 5 most important features:\")\nimportance_vals = np.abs(shap_values_np).mean(axis=0)\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance_vals\n})\nimportance_df = importance_df.sort_values('importance', ascending=False)\nfor i, row in importance_df.head(5).iterrows():\n    print(f\"- {row['feature']}: {row['importance']:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d3744fd-e634-4592-b8d0-dedd42f5a600",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 10. Production Pipeline Setup\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "e1c55915-135f-489f-92b0-91aea7defa68",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": "print(\"\\nSetting up production pipeline...\")\n\n# Create production feature store\nproduction_fs = FeatureStore(\n    session=session,\n    database=DATABASE_NAME,\n    default_warehouse=WAREHOUSE_NAME,\n    name=SCHEMA_NAME,\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\n# Get feature view for production\nexperiment_fv = production_fs.get_feature_view(\n    name=\"truck_sensors_feature_view_train\",\n    version=\"v1\"\n)\n\n# Extract SQL logic for feature engineering\nsql_logic = experiment_fv.feature_df.queries['queries'][0]\nprint(\"Extracted feature engineering SQL logic\")\n\n# Simplified production SQL that targets production data table\nproduction_sql = f\"\"\"\nSELECT \n    \"EXHAUST_GAS_TEMP\", \"OIL_PRESSURE\", \"BOOST_PRESSURE\", \"OIL_CONTAMINATION\", \n    \"ENGINE_BOOST_RATIO\", \"AMBIENT_TEMP\", \"RPM\", \"SPEED\", \"GPS_LAT\", \"GPS_LON\", \n    \"TIMESTAMP\", \"TRUCK_ID\", \n    \n    -- Lag features\n    \"EXHAUST_GAS_TEMP_LAG1\", \"OIL_PRESSURE_LAG1\", \"BOOST_PRESSURE_LAG1\", \n    \"OIL_CONTAMINATION_LAG1\", \"ENGINE_BOOST_RATIO_LAG1\", \n    \n    -- Rolling averages\n    \"EXHAUST_GAS_TEMP_AVG_1HR\", \"OIL_PRESSURE_AVG_1HR\", \"BOOST_PRESSURE_AVG_1HR\", \n    \"OIL_CONTAMINATION_AVG_1HR\", \"ENGINE_BOOST_RATIO_AVG_1HR\", \n    \n    -- Min/Max\n    \"EXHAUST_GAS_TEMP_MIN_1HR\", \"EXHAUST_GAS_TEMP_MAX_1HR\", \n    \"OIL_PRESSURE_MIN_1HR\", \"OIL_PRESSURE_MAX_1HR\", \n    \n    -- Delta calculations\n    (\"EXHAUST_GAS_TEMP\" - \"EXHAUST_GAS_TEMP_LAG1\") AS \"DELTA_EXHAUST_GAS_TEMP\", \n    (\"OIL_PRESSURE\" - \"OIL_PRESSURE_LAG1\") AS \"DELTA_OIL_PRESSURE\", \n    (\"BOOST_PRESSURE\" - \"BOOST_PRESSURE_LAG1\") AS \"DELTA_BOOST_PRESSURE\", \n    (\"OIL_CONTAMINATION\" - \"OIL_CONTAMINATION_LAG1\") AS \"DELTA_OIL_CONTAMINATION\", \n    (\"ENGINE_BOOST_RATIO\" - \"ENGINE_BOOST_RATIO_LAG1\") AS \"DELTA_ENGINE_BOOST_RATIO\", \n    \n    -- Time features\n    hour(\"TIMESTAMP\") AS \"HOUR_OF_DAY\" \nFROM (\n    SELECT \n        \"EXHAUST_GAS_TEMP\", \"OIL_PRESSURE\", \"BOOST_PRESSURE\", \"OIL_CONTAMINATION\", \n        \"ENGINE_BOOST_RATIO\", \"AMBIENT_TEMP\", \"RPM\", \"SPEED\", \"GPS_LAT\", \"GPS_LON\", \n        \"TIMESTAMP\", \"TRUCK_ID\", \n        \n        -- Lag calculations\n        LAG(\"EXHAUST_GAS_TEMP\", 1, NULL) OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\") AS \"EXHAUST_GAS_TEMP_LAG1\", \n        LAG(\"OIL_PRESSURE\", 1, NULL) OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\") AS \"OIL_PRESSURE_LAG1\", \n        LAG(\"BOOST_PRESSURE\", 1, NULL) OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\") AS \"BOOST_PRESSURE_LAG1\", \n        LAG(\"OIL_CONTAMINATION\", 1, NULL) OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\") AS \"OIL_CONTAMINATION_LAG1\", \n        LAG(\"ENGINE_BOOST_RATIO\", 1, NULL) OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\") AS \"ENGINE_BOOST_RATIO_LAG1\", \n        \n        -- Rolling averages (12 rows = ~1 hour of data)\n        AVG(\"EXHAUST_GAS_TEMP\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"EXHAUST_GAS_TEMP_AVG_1HR\", \n        AVG(\"OIL_PRESSURE\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"OIL_PRESSURE_AVG_1HR\", \n        AVG(\"BOOST_PRESSURE\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"BOOST_PRESSURE_AVG_1HR\", \n        AVG(\"OIL_CONTAMINATION\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"OIL_CONTAMINATION_AVG_1HR\", \n        AVG(\"ENGINE_BOOST_RATIO\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"ENGINE_BOOST_RATIO_AVG_1HR\", \n        \n        -- Min/Max values\n        MIN(\"EXHAUST_GAS_TEMP\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"EXHAUST_GAS_TEMP_MIN_1HR\", \n        MAX(\"EXHAUST_GAS_TEMP\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"EXHAUST_GAS_TEMP_MAX_1HR\", \n        MIN(\"OIL_PRESSURE\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"OIL_PRESSURE_MIN_1HR\", \n        MAX(\"OIL_PRESSURE\") OVER (PARTITION BY \"TRUCK_ID\" ORDER BY \"TIMESTAMP\" ROWS BETWEEN 12 PRECEDING AND CURRENT ROW) AS \"OIL_PRESSURE_MAX_1HR\" \n    FROM (\n        SELECT \n            \"EXHAUST_GAS_TEMP\", \"OIL_PRESSURE\", \"BOOST_PRESSURE\", \"OIL_CONTAMINATION\", \n            \"ENGINE_BOOST_RATIO\", \"AMBIENT_TEMP\", \"RPM\", \"SPEED\", \"GPS_LAT\", \"GPS_LON\", \n            CAST(\"TIMESTAMP\" AS TIMESTAMP) AS \"TIMESTAMP\", \n            CAST(\"TRUCK_ID\" AS INT) AS \"TRUCK_ID\" \n        FROM {PRODUCTION_DATA_TABLE}\n    )\n)\n\"\"\"\n\n# Create dataframe with production features\nfeature_df = session.sql(production_sql)\nprint(\"Created production feature dataframe\")\n\n# Register all entities from experiment feature view\nfor entity in experiment_fv.entities:\n    production_fs.register_entity(entity)\n\n# Create production feature view\nproduction_fv = FeatureView(\n    name=\"PRODUCTION_FEATURE_VIEW\",\n    entities=experiment_fv.entities,\n    feature_df=feature_df,\n    timestamp_col=\"TIMESTAMP\",\n    refresh_freq=\"5 minutes\",\n    desc=\"Production feature view for truck telemetry\"\n)\n\n# Register production feature view\nproduction_fs.register_feature_view(\n    feature_view=production_fv,\n    version=\"1\",\n    block=True,\n    overwrite=True\n)\n\nprint(\"Registered production feature view\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d319708a-6a5d-420c-acc2-b20fbff75270",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 11. Apply Model for Predictions\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "b3605187-8bcd-4a42-9082-3faa3e8abbed",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "print(\"\\nApplying model to production data...\")\n\n# Import warnings to suppress specific warnings\nimport warnings\n\n# Load model from registry\nregistry = Registry(session=session)\nmodel = registry.get_model(MODEL_NAME)\n\n# Get the specific version of the model you registered\nmodel_version_name = \"XGB_V1_20250515_155142\"  # Use your actual version name from registration\nmodel_version = model.version(model_version_name)\n\n# Create inference function\ndef predict_failure_probability(feature_vector, model):\n    \"\"\"Run inference with the specified model\"\"\"\n    return model.run(feature_vector, function_name=\"predict_proba\")\n\n# Read from production feature view\nfeature_view = production_fs.get_feature_view(\"PRODUCTION_FEATURE_VIEW\", version=\"1\")\ninference_input_sdf = production_fs.read_feature_view(feature_view)\n\n# Apply explicit decimal type conversion to inference data\nprint(\"Explicitly casting Decimal columns in inference data to Double...\")\ninference_input_sdf = explicitly_cast_decimal_columns(inference_input_sdf)\n\n# Make predictions with warning suppression\nprint(\"Generating predictions...\")\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", message=\".*Type DecimalType.*is being automatically converted to DOUBLE.*\")\n    inference_result_sdf = predict_failure_probability(inference_input_sdf, model_version)\n\nprint(\"Generated predictions on production data\")\n\n# Show sample predictions\ninference_result_sdf.sort(F.col('TRUCK_ID').desc(), F.col('TIMESTAMP')).show(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e283cd92-fcd2-43df-b074-ff1e6e3549f9",
   "metadata": {
    "name": "cell28",
    "collapsed": false
   },
   "source": "# ----------------------------------\n# 12. Save Predictions to Table\n# ----------------------------------"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "print(f\"\\nSaving predictions to {PREDICTION_OUTPUT_TABLE}...\")\ninference_result_sdf.write.mode(\"overwrite\").save_as_table(PREDICTION_OUTPUT_TABLE)\nprint(\"Predictions saved successfully!\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}
